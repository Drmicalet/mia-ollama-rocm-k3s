# ğŸ¦™ MIA Ollama ROCm K3s

Despliegue de Ollama con soporte ROCm para GPUs AMD en Kubernetes (K3s).

## ğŸ“‹ Requisitos

| Componente | VersiÃ³n | Notas |
|------------|---------|-------|
| K3s | v1.34+ | Kubernetes ligero |
| Longhorn | Latest | Almacenamiento distribuido |
| ROCm | 6.0+ | Drivers AMD GPU |
| GPU AMD | gfx1030 | RX 6950 XT / 6900 XT / 6800 XT |

## ğŸ–¥ï¸ Hardware Probado

- **CPU**: AMD Ryzen 7 9800X3D (8 cores)
- **GPU**: AMD RX 6950 XT (16GB VRAM)
- **RAM**: 64GB DDR5
- **SO**: CachyOS Linux (Arch-based)

## ğŸš€ InstalaciÃ³n RÃ¡pida

```bash
# Clonar repositorio
git clone https://github.com/Drmicalet/mia-ollama-rocm-k3s.git
cd mia-ollama-rocm-k3s

# Ejecutar instalador
./scripts/install.sh

# Verificar instalaciÃ³n
./scripts/verify.sh
```

## ğŸ“¦ Modelos Incluidos

| Modelo | TamaÃ±o | Uso Principal |
|--------|--------|---------------|
| glm4:9b | 5.5 GB | General |
| qwen2.5:7b | 4.7 GB | Multilingual |
| mistral:7b | 4.4 GB | General |
| gemma2:2b | 1.6 GB | RÃ¡pido |
| gemma2:9b | 5.4 GB | Calidad |

## ğŸ”§ Estructura

```
mia-ollama-rocm-k3s/
â”œâ”€â”€ k8s/
â”‚   â”œâ”€â”€ 00-namespace.yaml
â”‚   â”œâ”€â”€ 01-configmap.yaml
â”‚   â”œâ”€â”€ 02-pvc.yaml
â”‚   â”œâ”€â”€ 03-deployment.yaml
â”‚   â”œâ”€â”€ 04-service.yaml
â”‚   â”œâ”€â”€ 05-model-puller.yaml
â”‚   â”œâ”€â”€ 06-ingress.yaml
â”‚   â””â”€â”€ 07-networkpolicy.yaml
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ install.sh
â”‚   â”œâ”€â”€ verify.sh
â”‚   â””â”€â”€ backup.sh
â”œâ”€â”€ docs/
â””â”€â”€ README.md
```

## ğŸŒ Uso de la API

```bash
# Listar modelos
curl http://localhost:31434/api/tags

# Generar texto
curl http://localhost:31434/api/generate -d '{
  "model": "gemma2:2b",
  "prompt": "Hola, Â¿cÃ³mo estÃ¡s?"
}'

# Chat
curl http://localhost:31434/api/chat -d '{
  "model": "qwen2.5:7b",
  "messages": [
    {"role": "user", "content": "Explica la relatividad"}
  ]
}'
```

## âš¡ Rendimiento

Con ROCm habilitado en RX 6950 XT:

| Modelo | Velocidad (GPU) | Velocidad (CPU) |
|--------|-----------------|-----------------|
| gemma2:2b | ~50-60 t/s | ~8-12 t/s |
| mistral:7b | ~25-35 t/s | ~4-6 t/s |
| qwen2.5:7b | ~20-30 t/s | ~3-5 t/s |
| glm4:9b | ~15-20 t/s | ~2-4 t/s |

## ğŸ” VerificaciÃ³n de GPU

```bash
# Verificar que ROCm estÃ¡ funcionando
kubectl logs -n mia-ollama deployment/ollama-rocm | grep -i rocm

# DeberÃ­as ver algo como:
# llama_context: ROCm0 compute buffer size = 504.50 MiB
```

## ğŸ› ï¸ Troubleshooting

### GPU no detectada

```bash
# Verificar dispositivos
ls -la /dev/kfd /dev/dri

# Verificar permisos
sudo chmod 777 /dev/kfd /dev/dri/renderD128
```

### Pod no inicia

```bash
# Verificar logs
kubectl describe pod -n mia-ollama -l app=ollama-rocm

# Verificar eventos
kubectl get events -n mia-ollama --sort-by='.lastTimestamp'
```

## ğŸ“„ Licencia

MIT License - Libre para uso personal y comercial.

## ğŸ™ CrÃ©ditos

- [Ollama](https://ollama.ai/) - Motor de inferencia
- [ROCm](https://rocm.docs.amd.com/) - Plataforma GPU AMD
- [K3s](https://k3s.io/) - Kubernetes ligero
- [Longhorn](https://longhorn.io/) - Almacenamiento distribuido

---

**MIA v20a** - Sistema de IA Multinivel con ROCm
